{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AttentionLens","text":"<p>Interpreting the latent space representations of attention head outputs for Large Language Models (LLMs).</p> <p>To train attention lense, navigate to the <code>train/</code> dir and run the command <code>python train_pl.py</code>.</p> <p>PyTorch Lighting has been used to support distributed training, so you can also use <code>torch.distributed.run</code> to distribute training across nodes. More complete documentation is coming soon.</p> <p>Demos for how to use a lens to view the vocabulary latent space of a specific attention head can be found in the <code>demos/</code> dir. Again, better docs coming soon. </p>"},{"location":"#installation","title":"Installation","text":"<p>Requirements: python &gt;=3.7,&lt;3.11</p> <pre><code>git clone https://github.com/msakarvadia/AttentionLens.git\ncd AttentionLens\nconda create --name attnlens python==3.10\nconda activate attnlens\npip install -r requirements.txt\npip install .\n</code></pre>"},{"location":"#development","title":"Development","text":"<pre><code>git clone https://github.com/msakarvadia/AttentionLens.git\ncd AttentionLens\nconda create --name attnlens python==3.10\nconda activate attnlens\npip install -r requirements.txt\npip install -e . # editable installation\n</code></pre>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#citing-attentionlens","title":"Citing AttentionLens","text":"<p>If you use <code>AttentionLens</code> or any of this code in your work, please cite the following paper. <pre><code>@article{sakarvadia2023attention,\n  title={Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism},\n  author={Sakarvadia, Mansi and Khan, Arham and Ajith, Aswathy and Grzenda, Daniel and Hudson, Nathaniel and Bauer, Andr{\\'e} and Chard, Kyle and Foster, Ian},\n  journal={arXiv preprint arXiv:2310.16270},\n  year={2023},\n  note={Will appear in NeurIPS Attributing Model Behavior at Scale workshop.} \n}\n</code></pre></p> <pre><code>...\n</code></pre>"},{"location":"reference/","title":"attention_lens","text":"<p>Add some docs here.</p> <pre><code>flowchart LR\n    A--&gt;B\n    B--&gt;C\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>attention_lens</li> <li>data<ul> <li>get_data_pl</li> </ul> </li> <li>lens<ul> <li>base</li> <li>registry<ul> <li>lensA</li> <li>lensLlama2</li> </ul> </li> </ul> </li> <li>model<ul> <li>get_model</li> </ul> </li> <li>train<ul> <li>config</li> <li>lightning_lens</li> <li>train_lens</li> </ul> </li> </ul>"},{"location":"reference/data/","title":"data","text":""},{"location":"reference/data/get_data_pl/","title":"get_data_pl","text":""},{"location":"reference/data/get_data_pl/#attention_lens.data.get_data_pl.DataModule","title":"DataModule","text":"<pre><code>DataModule(name: str = 'bookcorpus', split: str = 'train', batch_size: int = 4, num_workers: int = 16, pin_memory: bool = True)\n</code></pre> <p>               Bases: <code>LightningDataModule</code></p> <p>Initializes a DataLoader object for \"bookcorpus\". Support for more datasets coming soon.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data  = DataModule()\n</code></pre>"},{"location":"reference/data/get_data_pl/#attention_lens.data.get_data_pl.DataModule.setup","title":"setup","text":"<pre><code>setup(stage) -&gt; None\n</code></pre> <p>Initializes a huggingface dataset: bookcorpus.</p>"},{"location":"reference/data/get_data_pl/#attention_lens.data.get_data_pl.DataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader() -&gt; DataLoader\n</code></pre> <p>Creates instance of <code>DataLoader</code>.</p> <p>Returns:</p> <ul> <li> <code>DataLoader</code>           \u2013            <p>A DataLoader for a specified dataset.</p> </li> </ul>"},{"location":"reference/lens/","title":"lens","text":"<p>To see all the implemented <code>Lens</code>, simply import <code>Lens</code> and print <code>Lens.registry</code>.</p> <pre><code>flowchart LR\n    Lens\n    subgraph Registry\n       LensA\n    end\n\n    Lens--&gt;LensA\n</code></pre>"},{"location":"reference/lens/#attention_lens.lens.Lens","title":"Lens","text":"<pre><code>Lens(unembed, bias, n_head, d_model, d_vocab)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>It is important to remember that subclasses of the <code>Lens</code> cannot take separate arguments as they will be ignored at runtime for instantiation in the <code>TransformerAttnInterface</code>.</p> <p>Parameters:</p> <ul> <li> <code>unembed</code>           \u2013            </li> <li> <code>bias</code>           \u2013            </li> <li> <code>n_head</code>           \u2013            </li> <li> <code>d_model</code>           \u2013            </li> <li> <code>d_vocab</code>           \u2013            </li> </ul>"},{"location":"reference/lens/#attention_lens.lens.Lens.get_lens","title":"get_lens  <code>classmethod</code>","text":"<pre><code>get_lens(name: str) -&gt; type[Lens]\n</code></pre> <p>This takes the name of the lens and queries the registry to grab the corresponding <code>Lens</code> subclas.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the child <code>Lens</code> implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Lens]</code>           \u2013            <p>Subclass <code>Lens</code> implementation.</p> </li> </ul>"},{"location":"reference/lens/#attention_lens.lens.LensA","title":"LensA","text":"<pre><code>LensA(unembed, bias, n_head, d_model, d_vocab)\n</code></pre> <p>               Bases: <code>Lens</code></p>"},{"location":"reference/lens/#attention_lens.lens.LensA.get_lens","title":"get_lens  <code>classmethod</code>","text":"<pre><code>get_lens(name: str) -&gt; type[Lens]\n</code></pre> <p>This takes the name of the lens and queries the registry to grab the corresponding <code>Lens</code> subclas.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the child <code>Lens</code> implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Lens]</code>           \u2013            <p>Subclass <code>Lens</code> implementation.</p> </li> </ul>"},{"location":"reference/lens/#attention_lens.lens.LensA.forward","title":"forward","text":"<pre><code>forward(input_tensor)\n</code></pre> <p>Performs a forward pass through the LensA model.</p> <p>Parameters:</p> <ul> <li> <code>input_tensor</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape (batch_size, pos, n_head, d_model).</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>torch.tensor: Output tensor of shape (batch_size, pos, d_vocab) after processing through</p> </li> <li>           \u2013            <p>the linear layers and summing across the attention heads.</p> </li> </ul>"},{"location":"reference/lens/base/","title":"base","text":""},{"location":"reference/lens/base/#attention_lens.lens.base.Lens","title":"Lens","text":"<pre><code>Lens(unembed, bias, n_head, d_model, d_vocab)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>It is important to remember that subclasses of the <code>Lens</code> cannot take separate arguments as they will be ignored at runtime for instantiation in the <code>TransformerAttnInterface</code>.</p> <p>Parameters:</p> <ul> <li> <code>unembed</code>           \u2013            </li> <li> <code>bias</code>           \u2013            </li> <li> <code>n_head</code>           \u2013            </li> <li> <code>d_model</code>           \u2013            </li> <li> <code>d_vocab</code>           \u2013            </li> </ul>"},{"location":"reference/lens/base/#attention_lens.lens.base.Lens.get_lens","title":"get_lens  <code>classmethod</code>","text":"<pre><code>get_lens(name: str) -&gt; type[Lens]\n</code></pre> <p>This takes the name of the lens and queries the registry to grab the corresponding <code>Lens</code> subclas.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the child <code>Lens</code> implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Lens]</code>           \u2013            <p>Subclass <code>Lens</code> implementation.</p> </li> </ul>"},{"location":"reference/lens/registry/","title":"registry","text":""},{"location":"reference/lens/registry/lensA/","title":"lensA","text":""},{"location":"reference/lens/registry/lensA/#attention_lens.lens.registry.lensA.LensA","title":"LensA","text":"<pre><code>LensA(unembed, bias, n_head, d_model, d_vocab)\n</code></pre> <p>               Bases: <code>Lens</code></p>"},{"location":"reference/lens/registry/lensA/#attention_lens.lens.registry.lensA.LensA.get_lens","title":"get_lens  <code>classmethod</code>","text":"<pre><code>get_lens(name: str) -&gt; type[Lens]\n</code></pre> <p>This takes the name of the lens and queries the registry to grab the corresponding <code>Lens</code> subclas.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the child <code>Lens</code> implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Lens]</code>           \u2013            <p>Subclass <code>Lens</code> implementation.</p> </li> </ul>"},{"location":"reference/lens/registry/lensA/#attention_lens.lens.registry.lensA.LensA.forward","title":"forward","text":"<pre><code>forward(input_tensor)\n</code></pre> <p>Performs a forward pass through the LensA model.</p> <p>Parameters:</p> <ul> <li> <code>input_tensor</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape (batch_size, pos, n_head, d_model).</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>torch.tensor: Output tensor of shape (batch_size, pos, d_vocab) after processing through</p> </li> <li>           \u2013            <p>the linear layers and summing across the attention heads.</p> </li> </ul>"},{"location":"reference/lens/registry/lensLlama2/","title":"lensLlama2","text":""},{"location":"reference/lens/registry/lensLlama2/#attention_lens.lens.registry.lensLlama2.LensLlama2","title":"LensLlama2","text":"<pre><code>LensLlama2(unembed, bias, n_head, d_model, d_vocab)\n</code></pre> <p>               Bases: <code>Lens</code></p>"},{"location":"reference/lens/registry/lensLlama2/#attention_lens.lens.registry.lensLlama2.LensLlama2.get_lens","title":"get_lens  <code>classmethod</code>","text":"<pre><code>get_lens(name: str) -&gt; type[Lens]\n</code></pre> <p>This takes the name of the lens and queries the registry to grab the corresponding <code>Lens</code> subclas.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the child <code>Lens</code> implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Lens]</code>           \u2013            <p>Subclass <code>Lens</code> implementation.</p> </li> </ul>"},{"location":"reference/lens/registry/lensLlama2/#attention_lens.lens.registry.lensLlama2.LensLlama2.forward","title":"forward","text":"<pre><code>forward(input_tensor)\n</code></pre> <p>This is optimizing \\(f(x) = \\sum_{...} ...\\) (TODO)</p> <p>Parameters:</p> <ul> <li> <code>input_tensor</code>           \u2013            </li> </ul> <p>Returns:</p>"},{"location":"reference/model/","title":"model","text":""},{"location":"reference/model/get_model/","title":"get_model","text":""},{"location":"reference/model/get_model/#attention_lens.model.get_model.get_model","title":"get_model","text":"<pre><code>get_model(model_name: str = 'gpt2', device: Union[str, torch.types.Device] = 'cuda') -&gt; AutoModelForCausalLM\n</code></pre> <p>Loads and returns a model and tokenizer from the modified Hugging Face Transformers library.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>'gpt2'</code> )           \u2013            <p>The name of the pre-trained model.</p> </li> <li> <code>device</code>               (<code>Union[str, Device]</code>, default:                   <code>'cuda'</code> )           \u2013            <p>The device to train on.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model, tokenizer = get_model(\"gpt2\")\n</code></pre> <p>Returns:</p> <ul> <li> <code>AutoModelForCausalLM</code>           \u2013            <p>The light-weight hooked model and tokenizer.</p> </li> </ul>"},{"location":"reference/train/","title":"train","text":""},{"location":"reference/train/config/","title":"config","text":""},{"location":"reference/train/lightning_lens/","title":"lightning_lens","text":""},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens","title":"LightningLens","text":"<pre><code>LightningLens(model_name: str, lens_cls: type[Lens] | str, layer_num: int, lr: float = 0.001, **kwargs)\n</code></pre> <p>               Bases: <code>LightningModule</code></p>"},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens.kl_loss","title":"kl_loss","text":"<pre><code>kl_loss(logits, lens_logits) -&gt; torch.Tensor\n</code></pre> <p>Compute the Kullback-Leibler divergence between tensors.</p> <p>Quantifies the difference between the probability distribution of the model's output versus the probability distribution of the attention lens.</p> \\[     D_{KL} (\\text{logits} \\Vert \\text{lens\\_logits}) \\] <p>Parameters:</p> <ul> <li> <code>logits</code>               (<code>Tensor[d_vocab]</code>)           \u2013            <p>A probability distribution of the model's outputs.</p> </li> <li> <code>lens_logits</code>               (<code>Tensor[d_vocab]</code>)           \u2013            <p>The output of the AttentionLens model acting on the entire layer from the attention mechanism.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            <p>(torch.Tensor[bsz]): Returns difference between logits and lens_logits</p> </li> </ul>"},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens.setup","title":"setup","text":"<pre><code>setup(stage) -&gt; None\n</code></pre> <p>Sets up the model and tokenizer during training setup.</p> <p>Parameters:</p> <ul> <li> <code>stage</code>           \u2013            <p>The stage of the training process.</p> </li> </ul>"},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens.forward","title":"forward","text":"<pre><code>forward(head_out) -&gt; torch.Tensor\n</code></pre> <p>Compute a forward pass through the Attention Lens</p> <p>Takes the hook information of an entire layer of the attention mechanism, and computes the forward pass through that layer of Transformer Lens models.</p> <p>Parameters:</p> <ul> <li> <code>head_out</code>               (<code>Tensor[bsz, q_len, d_model]</code>)           \u2013            <p>The hooked information of an entire layer of the attention mechanism.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>lens_out</code> (              <code>Tensor[bsz, d_vocab]</code> )          \u2013            <p>The prediction of the attention lens models for that layer.</p> </li> </ul>"},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens.training_step","title":"training_step","text":"<pre><code>training_step(train_batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor\n</code></pre> <p>Defines a single step in the training loop. Takes in an entire batch and computes the KL-loss for that batch.</p> <p>Parameters:</p> <ul> <li> <code>train_batch</code>               (<code>Tensor</code>)           \u2013            <p>The batch (bsz) of data for the current training</p> </li> <li> <code>batch_idx</code>               (<code>int</code>)           \u2013            <p>The index of the batch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: The loss for the current training step.</p> </li> </ul>"},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers() -&gt; torch.optim.Optimizer\n</code></pre> <p>Configures the optimizer for training.</p> <p>Returns:</p> <ul> <li> <code>Optimizer</code>           \u2013            <p>torch.optim.Optimizer: The optimizer for training.</p> </li> </ul>"},{"location":"reference/train/train_lens/","title":"train_lens","text":""},{"location":"reference/train/train_lens/#attention_lens.train.train_lens.train_lens","title":"train_lens","text":"<pre><code>train_lens(lens: LightningLens, data_module: DataModule, config: TrainConfig, callbacks: Optional[Union[list[Callback], Callback]] = None)\n</code></pre> <p>Trains the given lens model using the provided data module and training configuration.</p> <p>Parameters:</p> <ul> <li> <code>lens</code>               (<code>LightningLens</code>)           \u2013            <p>The LightningLens model to be trained.</p> </li> <li> <code>data_module</code>               (<code>DataModule</code>)           \u2013            <p>The DataModule providing the training and validation data.</p> </li> <li> <code>config</code>               (<code>TrainConfig</code>)           \u2013            <p>The configuration settings for training.</p> </li> <li> <code>callbacks</code>               (<code>Optional[Union[list[Callback], Callback]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of callbacks or</p> </li> </ul> Notes <ul> <li>The training precision is set to mixed precision (16-mixed) if config.mix_precision is True, otherwise 32-bit precision is used.</li> <li>The training uses a distributed data parallel strategy with unused parameter detection enabled. (Necessary for GPU training, incompatible with CPU training.)</li> <li>If no specific checkpoint to reload from is specified, the function searches for the most recent checkpoint in the checkpoint directory.</li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>trainer fits the lens according to data_module.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lens = LightningLens(config.model_name, \"lensa\", config.layer_number, config.lr)\n&gt;&gt;&gt; data = DataModule()\n&gt;&gt;&gt; train_lens(lens, data, config, callbacks=[checkpoint_callback, early_stop_callback])\n</code></pre>"}]}