{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AttentionLens","text":"<p>Interpreting the latent space representations of attention head outputs for Large Language Models (LLMs).</p> <p>To train Attention Lens, navigate to the <code>AttentionLens/</code> directory and run the command <code>python train.py</code>. For examples on training Attention Lens with PBS scheduler, navigate to the <code>AttentionLens/attention_lens/</code> and run the command <code>./experiments.sh</code>. For more information on Attention Lens, and training, see <code>Getting Started</code>.</p> <p>PyTorch Lighting has been used to support distributed training, so you can also use <code>torch.distributed.run</code> to distribute training across nodes. More complete documentation is coming soon.</p> <p>Demos for how to extract and use a lens to view the vocabulary latent space of a specific attention head can be found in the <code>demos/</code> directory. </p>"},{"location":"#installation","title":"Installation","text":"<p>Requirements: python &gt;=3.7,&lt;3.11</p> <pre><code>git clone https://github.com/msakarvadia/AttentionLens.git\ncd AttentionLens\nconda create --name attnlens python==3.10\nconda activate attnlens\npip install -r requirements.txt\npip install .\n</code></pre>"},{"location":"#development","title":"Development","text":"<pre><code>git clone https://github.com/msakarvadia/AttentionLens.git\ncd AttentionLens\nconda create --name attnlens python==3.10\nconda activate attnlens\npip install -r requirements.txt\npip install -e . # editable installation\n</code></pre>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#citing-attentionlens","title":"Citing AttentionLens","text":"<p>If you use <code>AttentionLens</code> or any of this code in your work, please cite the following paper. <pre><code>@article{sakarvadia2023attention,\n  title={Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism},\n  author={Sakarvadia, Mansi and Khan, Arham and Ajith, Aswathy and Grzenda, Daniel and Hudson, Nathaniel and Bauer, Andr{\\'e} and Chard, Kyle and Foster, Ian},\n  journal={arXiv preprint arXiv:2310.16270},\n  year={2023},\n  note={Will appear in NeurIPS Attributing Model Behavior at Scale workshop.} \n}\n</code></pre></p> <pre><code>...\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#overview","title":"Overview","text":"<p>This guide will provide a starting point with training and modifying Attention Lens.</p>"},{"location":"getting_started/#sections","title":"Sections","text":"<ol> <li>Introduction</li> <li>Running on Polaris</li> <li>Configs and Args</li> <li>Model Definition</li> <li>Training</li> </ol>"},{"location":"getting_started/config/","title":"Configs and Args","text":"<p>This section covers the configuration settings and argument parsing for the training scripts.</p>"},{"location":"getting_started/config/#configpy","title":"<code>config.py</code>","text":""},{"location":"getting_started/config/#description","title":"Description","text":"<p>The file <code>/AttentionLens/attention_lens/train/config.py</code> defines a <code>TrainConfig</code> data class, which holds the default configuration settings for training.</p>"},{"location":"getting_started/config/#variables","title":"Variables:","text":"<ul> <li><code>lr</code>: Learning rate for the optimizer.</li> <li><code>epochs</code>: Number of complete passes through the training set.</li> <li><code>max_checkpoint_num</code>: Maximum number of checkpoint files to keep.</li> <li><code>batch_size</code>: Number of samples processed before the model is updated.</li> <li><code>num_nodes</code>: Number of nodes to use in distributed training.</li> <li><code>mixed_precision</code>: Boolean flag to indicate if mixed precision training should be used.</li> <li><code>checkpoint_mode</code>: Mode to determine when to save checkpoints, either after a certain number of steps (<code>step</code>) or based on training loss (<code>loss</code>).</li> <li><code>num_steps_per_checkpoint</code>: Number of steps between checkpoints when <code>checkpoint_mode</code> is set to <code>step</code>.</li> <li><code>checkpoint_dir</code>: Directory where checkpoint files are saved.</li> <li><code>accumulate_grad_batches</code>: Number of steps to accumulate gradients before updating model parameters.</li> <li><code>reload_checkpoint</code>: Path to a checkpoint file to resume training from.</li> <li><code>stopping_delta</code>: Minimum change in loss to qualify as an improvement for early stopping.</li> <li><code>stopping_patience</code>: Number of checks with no improvement after which training is stopped.</li> <li><code>model_name</code>: Name of the model architecture to use.</li> <li><code>layer_number</code>: Specific layer number to start training from. </li> </ul>"},{"location":"getting_started/config/#usage","title":"Usage:","text":"<p><code>config = TrainConfig()</code></p>"},{"location":"getting_started/config/#load_argspy","title":"<code>load_args.py</code>","text":""},{"location":"getting_started/config/#description_1","title":"Description:","text":"<p>The file <code>/AttentionLens/load_args.py</code> uses the <code>argparse</code> module to parse command-line arguments. The parameters here are the same as those in <code>config.py</code>. Note that changes here will override all defaults set in <code>config.py</code> and <code>lightning_lens.py</code>. </p>"},{"location":"getting_started/config/#usage_1","title":"Usage:","text":"<p>To modify hyperparameters during the training of Attention Lens, use the <code>--</code> modifier. For example:</p> <p><code>python Train.py --lr 1e-3</code></p>"},{"location":"getting_started/lens_def/","title":"Lens Definition","text":"<p>This section covers the definition of the AttentionLens</p>"},{"location":"getting_started/lens_def/#basepy","title":"<code>base.py</code>","text":""},{"location":"getting_started/lens_def/#description","title":"Description:","text":"<p>The file <code>AttentionLens/attention_lens/lens/base.py</code> defines the base Lens class, which is used as a foundation for creating specific lens models.</p>"},{"location":"getting_started/lens_def/#key-components","title":"Key Components:","text":"<ul> <li><code>get_lens()</code>: Retrieves a lens class from the registry by name.</li> </ul>"},{"location":"getting_started/lens_def/#usage","title":"Usage:","text":"<p><code>lens_cls = Lens.get_lense(lens_cls)</code></p>"},{"location":"getting_started/lens_def/#lensapy","title":"<code>lensA.py</code>","text":""},{"location":"getting_started/lens_def/#description_1","title":"Description:","text":"<p>The file <code>AttentionLens/attention_lens/lens/registry/lensA.py</code> defines the LensA lens model. This particular lens is configured to create a lens for each head in a LM layer.</p>"},{"location":"getting_started/lens_def/#usage_1","title":"Usage:","text":"<p><code>lensA = LightningLens('gpt2', 'lensa', layer_num=7, lr=1e-3)</code></p>"},{"location":"getting_started/lens_def/#lightning_lenspy","title":"<code>lightning_lens.py</code>","text":""},{"location":"getting_started/lens_def/#description_2","title":"Description:","text":"<p>The file <code>AttentionLens/attention_lens/train/lightning_lens.py</code> prepares the Lens for training with <code>train_lens.py</code> by configuring the lens, loss function, forward passes and the optimizer.</p>"},{"location":"getting_started/lens_def/#key-components_1","title":"Key Components:","text":"<ul> <li><code>kl_loss</code>: Computes the Kullback-Leibler divergence loss between model logits and lens logits.</li> <li><code>setup</code>: Sets up the model and tokenizer during the training setup.</li> <li><code>forward</code>: Computes a forward pass through the Attention Lens.</li> <li><code>training_step</code>: Defines a single step in the training loop, and returns the resultant loss.</li> <li><code>configure_optimizer</code>: Configures the optimizer for training.</li> </ul>"},{"location":"getting_started/lens_def/#usage_2","title":"Usage:","text":"<p><code>lensA = LightningLens('gpt2', 'lensa', layer_num=7, lr=1e-3)</code></p>"},{"location":"getting_started/polaris/","title":"Running on Polaris","text":"<p>These scripts are responsible for submitting the training jobs via the PBS jobs scheduler.</p>"},{"location":"getting_started/polaris/#experimentssh","title":"<code>experiments.sh</code>","text":""},{"location":"getting_started/polaris/#description","title":"Description:","text":"<p>The <code>/AttentionLens/attention_lens/experiments.sh</code> file submits multiple jobs for each layer of the Attention Lens model.</p>"},{"location":"getting_started/polaris/#variables","title":"Variables:","text":"<ul> <li><code>model_name</code>: Defines the name of model in use.</li> <li><code>ckpt_dir</code>: Defines the checkpoint directory to save to. If this directory already exists, and contains a checkpoint, training will pickup from this checkpoint.</li> <li><code>job_name</code>: Creates a PBS job name for easy tracking.</li> <li><code>num_layer</code>: Number of layers that exist for a particular model.</li> <li><code>layer</code>: Defines the layer of the model that Attention Lens will be trained on.</li> </ul>"},{"location":"getting_started/polaris/#usage","title":"Usage:","text":"<p><code>./experiments.sh</code></p>"},{"location":"getting_started/polaris/#simple_submitpbs","title":"<code>simple_submit.pbs</code>","text":""},{"location":"getting_started/polaris/#description_1","title":"Description:","text":"<p>The <code>/AttentionLens/attention_lens/simple_submit.pbs</code> is responsible for setting up the environment, coordination for multi-node training, and PBS job submition.</p>"},{"location":"getting_started/polaris/#usage_1","title":"Usage:","text":"<p><code>qsub simple_submit.pbs</code></p>"},{"location":"getting_started/train/","title":"Training Attention  Lens","text":"<p>This section guides you through the process of training a lens model using <code>train_lens.py</code> and <code>train.py</code>.</p>"},{"location":"getting_started/train/#train_lenspy","title":"<code>train_lens.py</code>","text":""},{"location":"getting_started/train/#description","title":"Description:","text":"<p>The file <code>AttentionLens/attention_lens/train/train_lens.py</code> contains the <code>train_lens()</code> function, which trains the given lens model using the specified data module and training configuration.</p>"},{"location":"getting_started/train/#key-components","title":"Key Components:","text":"<ul> <li><code>train_lens</code>: Handles the training process for the given lens model.</li> <li><code>training_precision</code>: Defines training precision to be used based on config options.</li> <li><code>strategy</code>: Parameter to enable distributed data parallel strategy with unused parameter detection.</li> <li>Checkpoint Handling: Searches for the most recent checkpoint if no specific checkpoint is provided.</li> </ul>  Notes <p> <ul> <li>The training precision is set to mixed precision (16-mixed) if config.mixed_precision is True, otherwise 32-bit precision is used.</li> <li>The training uses a distributed data parallel strategy with unused parameter detection enabled: <code>strategy=\"ddp_find_unused_parameters_true\"</code>. (Necessary for GPU training, incompatible with CPU training.)</li> <li>If no specific checkpoint to reload from is specified, the function searches for the most recent checkpoint in the checkpoint directory.</li> </ul> </p>"},{"location":"getting_started/train/#usage","title":"Usage:","text":"<p><code>train_lens(lens, data, config, callbacks=callbacks)</code></p>"},{"location":"getting_started/train/#trainpy","title":"<code>train.py</code>","text":""},{"location":"getting_started/train/#description_1","title":"Description:","text":"<p>The file <code>AttentionLens/train.py</code> sets up the training configuration, initializes the model, data module and lens, and calls the <code>train_lens</code> function to begin the training process.</p>"},{"location":"getting_started/train/#usage_1","title":"Usage","text":"<p><code>python train.py --lr 1e-3 --epochs 5 --batch_size 32 --num_nodes 2</code></p> <p>See also: Running on Polaris.</p>"},{"location":"reference/","title":"attention_lens","text":"<p>Add some docs here.</p> <pre><code>flowchart LR\n    A--&gt;B\n    B--&gt;C\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>attention_lens</li> <li>data<ul> <li>get_data_pl</li> </ul> </li> <li>lens<ul> <li>base</li> <li>registry<ul> <li>lensA</li> <li>lensLlama2</li> </ul> </li> </ul> </li> <li>model<ul> <li>get_model</li> </ul> </li> <li>train<ul> <li>config</li> <li>lightning_lens</li> <li>train_lens</li> </ul> </li> </ul>"},{"location":"reference/data/","title":"data","text":""},{"location":"reference/data/get_data_pl/","title":"get_data_pl","text":""},{"location":"reference/data/get_data_pl/#attention_lens.data.get_data_pl.DataModule","title":"DataModule","text":"<pre><code>DataModule(name: str = 'bookcorpus', split: str = 'train', batch_size: int = 4, num_workers: int = 16, pin_memory: bool = True)\n</code></pre> <p>               Bases: <code>LightningDataModule</code></p> <p>Initializes a DataLoader object for \"bookcorpus\". Support for more datasets coming soon.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data  = DataModule()\n</code></pre>"},{"location":"reference/data/get_data_pl/#attention_lens.data.get_data_pl.DataModule.setup","title":"setup","text":"<pre><code>setup(stage) -&gt; None\n</code></pre> <p>Initializes a huggingface dataset: bookcorpus.</p>"},{"location":"reference/data/get_data_pl/#attention_lens.data.get_data_pl.DataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader() -&gt; DataLoader\n</code></pre> <p>Creates instance of <code>DataLoader</code>.</p> <p>Returns:</p> <ul> <li> <code>DataLoader</code>           \u2013            <p>A DataLoader for a specified dataset.</p> </li> </ul>"},{"location":"reference/lens/","title":"lens","text":"<p>To see all the implemented <code>Lens</code>, simply import <code>Lens</code> and print <code>Lens.registry</code>.</p> <pre><code>flowchart LR\n    Lens\n    subgraph Registry\n       LensA\n    end\n\n    Lens--&gt;LensA\n</code></pre>"},{"location":"reference/lens/#attention_lens.lens.Lens","title":"Lens","text":"<pre><code>Lens(unembed, bias, n_head, d_model, d_vocab)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>It is important to remember that subclasses of the <code>Lens</code> cannot take separate arguments as they will be ignored at runtime for instantiation in the <code>TransformerAttnInterface</code>.</p> <p>Parameters:</p> <ul> <li> <code>unembed</code>           \u2013            </li> <li> <code>bias</code>           \u2013            </li> <li> <code>n_head</code>           \u2013            </li> <li> <code>d_model</code>           \u2013            </li> <li> <code>d_vocab</code>           \u2013            </li> </ul>"},{"location":"reference/lens/#attention_lens.lens.Lens.get_lens","title":"get_lens  <code>classmethod</code>","text":"<pre><code>get_lens(name: str) -&gt; type[Lens]\n</code></pre> <p>This takes the name of the lens and queries the registry to grab the corresponding <code>Lens</code> subclas.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the child <code>Lens</code> implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Lens]</code>           \u2013            <p>Subclass <code>Lens</code> implementation.</p> </li> </ul>"},{"location":"reference/lens/#attention_lens.lens.LensA","title":"LensA","text":"<pre><code>LensA(unembed, bias, n_head, d_model, d_vocab)\n</code></pre> <p>               Bases: <code>Lens</code></p>"},{"location":"reference/lens/#attention_lens.lens.LensA.get_lens","title":"get_lens  <code>classmethod</code>","text":"<pre><code>get_lens(name: str) -&gt; type[Lens]\n</code></pre> <p>This takes the name of the lens and queries the registry to grab the corresponding <code>Lens</code> subclas.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the child <code>Lens</code> implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Lens]</code>           \u2013            <p>Subclass <code>Lens</code> implementation.</p> </li> </ul>"},{"location":"reference/lens/#attention_lens.lens.LensA.forward","title":"forward","text":"<pre><code>forward(input_tensor)\n</code></pre> <p>Performs a forward pass through the LensA model.</p> <p>Parameters:</p> <ul> <li> <code>input_tensor</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape (batch_size, pos, n_head, d_model).</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>torch.tensor: Output tensor of shape (batch_size, pos, d_vocab) after processing through</p> </li> <li>           \u2013            <p>the linear layers and summing across the attention heads.</p> </li> </ul>"},{"location":"reference/lens/base/","title":"base","text":""},{"location":"reference/lens/base/#attention_lens.lens.base.Lens","title":"Lens","text":"<pre><code>Lens(unembed, bias, n_head, d_model, d_vocab)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>It is important to remember that subclasses of the <code>Lens</code> cannot take separate arguments as they will be ignored at runtime for instantiation in the <code>TransformerAttnInterface</code>.</p> <p>Parameters:</p> <ul> <li> <code>unembed</code>           \u2013            </li> <li> <code>bias</code>           \u2013            </li> <li> <code>n_head</code>           \u2013            </li> <li> <code>d_model</code>           \u2013            </li> <li> <code>d_vocab</code>           \u2013            </li> </ul>"},{"location":"reference/lens/base/#attention_lens.lens.base.Lens.get_lens","title":"get_lens  <code>classmethod</code>","text":"<pre><code>get_lens(name: str) -&gt; type[Lens]\n</code></pre> <p>This takes the name of the lens and queries the registry to grab the corresponding <code>Lens</code> subclas.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the child <code>Lens</code> implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Lens]</code>           \u2013            <p>Subclass <code>Lens</code> implementation.</p> </li> </ul>"},{"location":"reference/lens/registry/","title":"registry","text":""},{"location":"reference/lens/registry/lensA/","title":"lensA","text":""},{"location":"reference/lens/registry/lensA/#attention_lens.lens.registry.lensA.LensA","title":"LensA","text":"<pre><code>LensA(unembed, bias, n_head, d_model, d_vocab)\n</code></pre> <p>               Bases: <code>Lens</code></p>"},{"location":"reference/lens/registry/lensA/#attention_lens.lens.registry.lensA.LensA.get_lens","title":"get_lens  <code>classmethod</code>","text":"<pre><code>get_lens(name: str) -&gt; type[Lens]\n</code></pre> <p>This takes the name of the lens and queries the registry to grab the corresponding <code>Lens</code> subclas.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the child <code>Lens</code> implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Lens]</code>           \u2013            <p>Subclass <code>Lens</code> implementation.</p> </li> </ul>"},{"location":"reference/lens/registry/lensA/#attention_lens.lens.registry.lensA.LensA.forward","title":"forward","text":"<pre><code>forward(input_tensor)\n</code></pre> <p>Performs a forward pass through the LensA model.</p> <p>Parameters:</p> <ul> <li> <code>input_tensor</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape (batch_size, pos, n_head, d_model).</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>torch.tensor: Output tensor of shape (batch_size, pos, d_vocab) after processing through</p> </li> <li>           \u2013            <p>the linear layers and summing across the attention heads.</p> </li> </ul>"},{"location":"reference/lens/registry/lensLlama2/","title":"lensLlama2","text":""},{"location":"reference/lens/registry/lensLlama2/#attention_lens.lens.registry.lensLlama2.LensLlama2","title":"LensLlama2","text":"<pre><code>LensLlama2(unembed, bias, n_head, d_model, d_vocab)\n</code></pre> <p>               Bases: <code>Lens</code></p>"},{"location":"reference/lens/registry/lensLlama2/#attention_lens.lens.registry.lensLlama2.LensLlama2.get_lens","title":"get_lens  <code>classmethod</code>","text":"<pre><code>get_lens(name: str) -&gt; type[Lens]\n</code></pre> <p>This takes the name of the lens and queries the registry to grab the corresponding <code>Lens</code> subclas.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the child <code>Lens</code> implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Lens]</code>           \u2013            <p>Subclass <code>Lens</code> implementation.</p> </li> </ul>"},{"location":"reference/lens/registry/lensLlama2/#attention_lens.lens.registry.lensLlama2.LensLlama2.forward","title":"forward","text":"<pre><code>forward(input_tensor)\n</code></pre> <p>This is optimizing \\(f(x) = \\sum_{...} ...\\) (TODO)</p> <p>Parameters:</p> <ul> <li> <code>input_tensor</code>           \u2013            </li> </ul> <p>Returns:</p>"},{"location":"reference/model/","title":"model","text":""},{"location":"reference/model/get_model/","title":"get_model","text":""},{"location":"reference/model/get_model/#attention_lens.model.get_model.get_model","title":"get_model","text":"<pre><code>get_model(model_name: str = 'gpt2', device: Union[str, torch.types.Device] = 'cuda') -&gt; AutoModelForCausalLM\n</code></pre> <p>Loads and returns a model and tokenizer from the modified Hugging Face Transformers library.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>'gpt2'</code> )           \u2013            <p>The name of the pre-trained model.</p> </li> <li> <code>device</code>               (<code>Union[str, Device]</code>, default:                   <code>'cuda'</code> )           \u2013            <p>The device to train on.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model, tokenizer = get_model(\"gpt2\")\n</code></pre> <p>Returns:</p> <ul> <li> <code>AutoModelForCausalLM</code>           \u2013            <p>The light-weight hooked model and tokenizer.</p> </li> </ul>"},{"location":"reference/train/","title":"train","text":""},{"location":"reference/train/config/","title":"config","text":""},{"location":"reference/train/lightning_lens/","title":"lightning_lens","text":""},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens","title":"LightningLens","text":"<pre><code>LightningLens(model_name: str, lens_cls: type[Lens] | str, layer_num: int, lr: float = 0.001, **kwargs)\n</code></pre> <p>               Bases: <code>LightningModule</code></p>"},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens.kl_loss","title":"kl_loss","text":"<pre><code>kl_loss(logits, lens_logits) -&gt; torch.Tensor\n</code></pre> <p>Compute the Kullback-Leibler divergence between tensors.</p> <p>Quantifies the difference between the probability distribution of the model's output versus the probability distribution of the attention lens.</p> \\[     D_{KL} (\\text{logits} \\Vert \\text{lens\\_logits}) \\] <p>Parameters:</p> <ul> <li> <code>logits</code>               (<code>Tensor[d_vocab]</code>)           \u2013            <p>A probability distribution of the model's outputs.</p> </li> <li> <code>lens_logits</code>               (<code>Tensor[d_vocab]</code>)           \u2013            <p>The output of the AttentionLens model acting on the entire layer from the attention mechanism.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            <p>(torch.Tensor[bsz]): Returns difference between logits and lens_logits</p> </li> </ul>"},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens.setup","title":"setup","text":"<pre><code>setup(stage) -&gt; None\n</code></pre> <p>Sets up the model and tokenizer during training setup.</p> <p>Parameters:</p> <ul> <li> <code>stage</code>           \u2013            <p>The stage of the training process.</p> </li> </ul>"},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens.forward","title":"forward","text":"<pre><code>forward(cache) -&gt; torch.Tensor\n</code></pre> <p>Compute a forward pass through the Attention Lens</p> <p>Takes the hook information of an entire layer of the attention mechanism, and computes the forward pass through that layer of Transformer Lens models.</p> <p>Args:</p> <pre><code>cache (torch.Tensor[bsz, q_len, d_model]): The hooked information of an\n\n    entire layer of the attention mechanism.\n</code></pre> <p>Returns:</p> <ul> <li> <code>lens_out</code> (              <code>Tensor[bsz, d_vocab]</code> )          \u2013            <p>The prediction of the attention lens models for that layer.</p> </li> </ul>"},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens.training_step","title":"training_step","text":"<pre><code>training_step(train_batch: torch.Tensor, batch_idx: int) -&gt; torch.Tensor\n</code></pre> <p>Defines a single step in the training loop. Takes in an entire batch and computes the KL-loss for that batch.</p> <p>Parameters:</p> <ul> <li> <code>train_batch</code>               (<code>Tensor</code>)           \u2013            <p>The batch (bsz) of data for the current training</p> </li> <li> <code>batch_idx</code>               (<code>int</code>)           \u2013            <p>The index of the batch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: The loss for the current training step.</p> </li> </ul>"},{"location":"reference/train/lightning_lens/#attention_lens.train.lightning_lens.LightningLens.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers() -&gt; torch.optim.Optimizer\n</code></pre> <p>Configures the optimizer for training.</p> <p>Returns:</p> <ul> <li> <code>Optimizer</code>           \u2013            <p>torch.optim.Optimizer: The optimizer for training.</p> </li> </ul>"},{"location":"reference/train/train_lens/","title":"train_lens","text":""},{"location":"reference/train/train_lens/#attention_lens.train.train_lens.train_lens","title":"train_lens","text":"<pre><code>train_lens(lens: LightningLens, data_module: DataModule, config: TrainConfig, callbacks: Optional[Union[list[Callback], Callback]] = None)\n</code></pre> <p>Trains the given lens model using the provided data module and training configuration.</p> <p>Parameters:</p> <ul> <li> <code>lens</code>               (<code>LightningLens</code>)           \u2013            <p>The LightningLens model to be trained.</p> </li> <li> <code>data_module</code>               (<code>DataModule</code>)           \u2013            <p>The DataModule providing the training and validation data.</p> </li> <li> <code>config</code>               (<code>TrainConfig</code>)           \u2013            <p>The configuration settings for training.</p> </li> <li> <code>callbacks</code>               (<code>Optional[Union[list[Callback], Callback]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of callbacks or</p> </li> </ul> Notes <ul> <li>The training precision is set to mixed precision (16-mixed) if config.mix_precision is True, otherwise 32-bit precision is used.</li> <li>The training uses a distributed data parallel strategy with unused parameter detection enabled. (Necessary for GPU training, incompatible with CPU training.)</li> <li>If no specific checkpoint to reload from is specified, the function searches for the most recent checkpoint in the checkpoint directory.</li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>trainer fits the lens according to data_module.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lens = LightningLens(config.model_name, \"lensa\", config.layer_number, config.lr)\n&gt;&gt;&gt; data = DataModule()\n&gt;&gt;&gt; train_lens(lens, data, config, callbacks=[checkpoint_callback, early_stop_callback])\n</code></pre>"}]}