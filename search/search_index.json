{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AttentionLens","text":"<p>Interpreting the latent space representations of attention head outputs for Large Language Models (LLMs).</p> <p>To train attention lense, navigate to the <code>train/</code> dir and run the command <code>python train_pl.py</code>.</p> <p>PyTorch Lighting has been used to support distributed training, so you can also use <code>torch.distributed.run</code> to distribute training across nodes. More complete documentation is coming soon.</p> <p>Demos for how to use a lens to view the vocabulary latent space of a specific attention head can be found in the <code>demos/</code> dir. Again, better docs coming soon. </p>"},{"location":"#installation","title":"Installation","text":"<p>Requirements: python &gt;=3.7,&lt;3.11</p> <pre><code>git clone https://github.com/msakarvadia/AttentionLens.git\ncd AttentionLens\nconda create --name attnlens python==3.10\nconda activate attnlens\npip install -r requirements.txt\npip install .\n</code></pre>"},{"location":"#development","title":"Development","text":"<pre><code>git clone https://github.com/msakarvadia/AttentionLens.git\ncd AttentionLens\nconda create --name attnlens python==3.10\nconda activate attnlens\npip install -r requirements.txt\npip install -e . # editable installation\n</code></pre>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#citing-attentionlens","title":"Citing AttentionLens","text":"<p>If you use AttentionLens or any of this code in your work, please cite the following paper.</p> <p>...</p> <pre><code>...\n</code></pre>"},{"location":"reference/","title":"attention_lense","text":""},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>attention_lense</li> <li>data<ul> <li>get_data_pl</li> </ul> </li> <li>lense<ul> <li>get_lense</li> <li>lenseA</li> </ul> </li> <li>model<ul> <li>get_model</li> </ul> </li> <li>train<ul> <li>train_pl</li> </ul> </li> </ul>"},{"location":"reference/data/","title":"data","text":""},{"location":"reference/data/get_data_pl/","title":"get_data_pl","text":""},{"location":"reference/lense/","title":"lense","text":""},{"location":"reference/lense/get_lense/","title":"get_lense","text":""},{"location":"reference/lense/lenseA/","title":"lenseA","text":""},{"location":"reference/model/","title":"model","text":""},{"location":"reference/model/get_model/","title":"get_model","text":""},{"location":"reference/model/get_model/#attention_lense.model.get_model.get_model","title":"get_model","text":"<pre><code>get_model(model_name: str = 'gpt2-small', device: Union[str, torch.types.Device] = 'cuda') -&gt; HookedTransformer\n</code></pre> <p>Loads and returns a pre-trained model from the HookedTransformer library by the given <code>name</code>.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>             (<code>str</code>, default:                 <code>'gpt2-small'</code> )         \u2013          <p>The name of the pre-trained model.</p> </li> <li> <code>device</code>             (<code>Union[str, Device]</code>, default:                 <code>'cuda'</code> )         \u2013          <p>The device to train on.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = get_model(model=\"gpt2-small\")\n</code></pre> <p>Returns:</p> <ul> <li> <code>HookedTransformer</code>         \u2013          <p>The pre-trained model with hooks.</p> </li> </ul>"},{"location":"reference/train/","title":"train","text":""},{"location":"reference/train/train_pl/","title":"train_pl","text":""}]}