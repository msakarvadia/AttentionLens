#!/bin/bash 
#PBS -l select=10
#PBS -l walltime=10:00:00
#PBS -q debug
#PBS -l filesystems=home:grand:eagle
#PBS -A SuperBERT

cd "/home/pettyjohnjn/AttentionLens/attention_lens"
echo "working dir: "
pwd

# Define the variables for your script
l_num=9
model_name="gpt2"
ckpt="/home/pettyjohnjn/AttentionLens/checkpoint/${model_name}/${l_num}/ckpt_"


# Figure out training environment
if [[ -z "${PBS_NODEFILE}" ]]; then
    RANKS=$HOSTNAME
    NNODES=1
else
    MASTER_RANK=$(head -n 1 $PBS_NODEFILE)
    echo "Master Rank: "$MASTER_RANK
    RANKS=$(tr '\n' ' ' < $PBS_NODEFILE)
    NNODES=$(< $PBS_NODEFILE wc -l)
fi

# Commands to run prior to the Python script for setting up the environment
PRELOAD+="module use /soft/modulefiles; module load conda; conda activate AttnLens_Test;  "
PRELOAD+="export OMP_NUM_THREADS=8 ; "
PRELOAD+="source ~/.bashrc ; "

# time python process to ensure timely job exit
TIMER="timeout 718m "

# torchrun launch configuration
LAUNCHER="python -m torch.distributed.run "
LAUNCHER+="--nnodes=$NNODES --nproc_per_node=auto --max_restarts 0 " 
if [[ "$NNODES" -eq 1 ]]; then
    LAUNCHER+="--standalone "
else
    LAUNCHER+="--master_port=1222 --master_addr=$MASTER_RANK "
fi

# Training script and parameters
echo "ckpt dir: "$ckpt
echo "layer num: "$l_num
echo "model name: "$model_name
CMD="../train.py --num_nodes $NNODES --model_name $model_name --checkpoint_dir $ckpt --layer_number $l_num"

RANK=0
for NODE in $RANKS; do
    NODERANK=" --node_rank=$RANK "
    FULL_CMD=" $PRELOAD $LAUNCHER $NODERANK $CMD $@ "
    echo "Training Command: $FULL_CMD"

    if [[ "$NODE" == "$HOSTNAME" ]]; then
        echo "Launching rank $RANK on local node $NODE"
        eval $FULL_CMD &
    else
        echo "Launching rank $RANK on remote node $NODE"
        ssh $NODE "cd $PWD; $FULL_CMD" &
    fi
    RANK=$((RANK+1))
done

wait